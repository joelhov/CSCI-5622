---
title: "clustering"
author: "Joel Hoversten"
date: "2024-12-09"
output: html_document
---

```{r}
library(tidyverse)
library(cluster)
library(stats)
library(NbClust)
library(mclust)
library(amap)  
library(factoextra) 
library(purrr)
library(stylo)  
library(philentropy)  
library(SnowballC)
library(caTools)
library(dplyr)
library(textstem)
library(stringr)
library(wordcloud)
library(tm) 
library(amap)
```



```{r}
setwd("C:/Users/joelh/OneDrive/Desktop/CSCI5622/Proj")
Record_3D_DF_all<-read.csv("ClusteringProj.csv")
Record_3D_DF<-Record_3D_DF_all  
head(Record_3D_DF)
str(Record_3D_DF)
(Label_3D <- Record_3D_DF$Platform)
Record_3D_DF <- Record_3D_DF[ ,-c(1) ]
head(Record_3D_DF)


(Dist1<- dist(Record_3D_DF, method = "minkowski", p=1)) ##Manhattan
(Dist2<- dist(Record_3D_DF, method = "minkowski", p=2)) #Euclidean
(DistE<- dist(Record_3D_DF, method = "euclidean")) #same as p = 2


(Record_3D_DF_Norm <- as.data.frame(apply(Record_3D_DF[,1:3 ], 2, ##2 for col
                                 function(x) (x - min(x))/(max(x)-min(x)))))


(Dist_norm<- dist(Record_3D_DF_Norm, method = "minkowski", p=2)) #Euclidean

```

```{r}
kmeans_3D_1<-NbClust::NbClust(Record_3D_DF_Norm, 
                             min.nc=2, max.nc=5, method="kmeans")
table(kmeans_3D_1$Best.n[1,])

barplot(table(kmeans_3D_1$Best.n[1,]), 
        xlab="Number of Clusters", ylab="",
        main="Number of Clusters")

fviz_nbclust(Record_3D_DF_Norm, method = "silhouette", 
                      FUN = hcut, k.max = 5)

```

```{r}
##############################
## Elbow Method (WSS - within sum sq)
############################# Elbow Methods ###################

fviz_nbclust(
  as.matrix(Record_3D_DF_Norm), 
  kmeans, 
  k.max = 5,
  method = "wss",
  diss = get_dist(as.matrix(Record_3D_DF_Norm), method = "manhattan")
)
```

```{r}
##########################
## k means..............
######################################
kmeans_3D_1_Result <- kmeans(Record_3D_DF, 2, nstart=25)   
## I could have used the normalized data - which is better to use
## But - by using the non-norm data, the results make more visual
## sense - which also matters.

# Print the results
print(kmeans_3D_1_Result)

kmeans_3D_1_Result$centers  

aggregate(Record_3D_DF, 
          by=list(cluster=kmeans_3D_1_Result$cluster), mean)


summary(kmeans_3D_1_Result)


## Place results in a tbale with the original data
cbind(Record_3D_DF_all, cluster = kmeans_3D_1_Result$cluster)

## See each cluster
kmeans_3D_1_Result$cluster

## This is the size (the number of points in) each cluster
# Cluster size
kmeans_3D_1_Result$size
## Here we have two clusters, each with 5 points (rows/vectors) 

## Visualize the clusters
fviz_cluster(kmeans_3D_1_Result, Record_3D_DF, main="Euclidean")
##-------------------------------------------------
## There are other k means options in R
## Let's try amap  Kmeans
## Notice the "K" in Kmeans is cap...
## k = 2
##RE:
## https://rdrr.io/cran/amap/man/Kmeans.html
##-----------------------------------------------------
My_Kmeans_3D_2<-Kmeans(Record_3D_DF_Norm, centers=2 ,method = "spearman")
fviz_cluster(My_Kmeans_3D_2, Record_3D_DF, main="Spearman")
## k= 3
My_Kmeans_3D_3<-Kmeans(Record_3D_DF_Norm, centers=3 ,method = "spearman")
fviz_cluster(My_Kmeans_3D_3, Record_3D_DF, main="Spearman")
My_Kmeans_3D_3<-Kmeans(Record_3D_DF_Norm, centers=4 ,method = "spearman")
fviz_cluster(My_Kmeans_3D_3, Record_3D_DF, main="Spearman")
## k = 2 with Euclidean
My_Kmeans_3D_E<-Kmeans(Record_3D_DF_Norm, centers=2 ,method = "euclidean")
fviz_cluster(My_Kmeans_3D_E, Record_3D_DF, main="Euclidean")
## k = 3 with Euclidean
My_Kmeans_3D_E3<-Kmeans(Record_3D_DF_Norm, centers=3 ,method = "euclidean")
fviz_cluster(My_Kmeans_3D_E3, Record_3D_DF, main="Euclidean")
My_Kmeans_3D_E3<-Kmeans(Record_3D_DF_Norm, centers=4 ,method = "euclidean")
fviz_cluster(My_Kmeans_3D_E3, Record_3D_DF, main="Euclidean")


## Heat maps...
## Recall that we have Dist2..
Dist2<- dist(Record_3D_DF, method = "minkowski", p=2) 
fviz_dist(Dist2, gradient = list(low = "#00AFBB", 
                            mid = "white", high = "#FC4E07"))+
                            ggtitle("Euclidean Heatmap")

## Compare to clusters...
cbind(Record_3D_DF_all, cluster = kmeans_3D_1_Result$cluster)

```


```{r}

#######################################################
## 
##          Hierarchical CLustering
## 
##
#######################################################
#
# Hierarchical clustering with Ward
# https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/hclust
#  
# ward.D2" = Ward's minimum variance method -
# however dissimilarities are **squared before clustering. 
# "single" = Nearest neighbours method. 
# "complete" = distance between two clusters is defined 
# as the maximum distance between an observation in one.
####################################################################
##
## For hclust, you need a distance matrix
## You can create any distance matrix you wish...
##
## https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/hclust
####################################################################
## Example:
(Dist_norm_M2<- dist(Record_3D_DF_Norm, method = "minkowski", p=2)) #Euclidean
## Now run hclust...you may use many methods - Ward, Ward.D2, complete, etc..
## see above
(HClust_Ward_Euc_N_3D <- hclust(Dist_norm_M2, method = "average" ))
plot(HClust_Ward_Euc_N_3D, cex=0.9, hang=-1, main = "Minkowski p=2 (Euclidean)")
rect.hclust(HClust_Ward_Euc_N_3D, k=2)

(HClust_Ward_Euc_N_3D <- hclust(Dist_norm_M2, method = "average" ))
plot(HClust_Ward_Euc_N_3D, cex=0.9, hang=-1, main = "Minkowski p=2 (Euclidean)")
rect.hclust(HClust_Ward_Euc_N_3D, k=3)

(HClust_Ward_Euc_N_3D <- hclust(Dist_norm_M2, method = "average" ))
plot(HClust_Ward_Euc_N_3D, cex=0.9, hang=-1, main = "Minkowski p=2 (Euclidean)")
rect.hclust(HClust_Ward_Euc_N_3D, k=4)

## Using Man with Ward.D2..............................
dist_C <- stats::dist(Record_3D_DF_Norm, method="manhattan")
HClust_Ward_CosSim_N_3D <- hclust(dist_C, method="ward.D2")
plot(HClust_Ward_CosSim_N_3D, cex=.7, hang=-30,main = "Manhattan")
rect.hclust(HClust_Ward_CosSim_N_3D, k=2)

dist_C <- stats::dist(Record_3D_DF_Norm, method="manhattan")
HClust_Ward_CosSim_N_3D <- hclust(dist_C, method="ward.D2")
plot(HClust_Ward_CosSim_N_3D, cex=.7, hang=-30,main = "Manhattan")
rect.hclust(HClust_Ward_CosSim_N_3D, k=3)

dist_C <- stats::dist(Record_3D_DF_Norm, method="manhattan")
HClust_Ward_CosSim_N_3D <- hclust(dist_C, method="ward.D2")
plot(HClust_Ward_CosSim_N_3D, cex=.7, hang=-30,main = "Manhattan")
rect.hclust(HClust_Ward_CosSim_N_3D, k=4)

## --------------------------------------------------------------------
```

```{r}
library(dplyr)       
library(ggplot2)     

library(cluster)     
library(factoextra)  


filename="C:/Users/joelh/OneDrive/Desktop/CSCI5622/Proj/ClusteringProj.csv"
HeartDF2<-read.csv(filename)
head(HeartDF2)
str(HeartDF2)
summary(HeartDF2)
```

```{r}



## !!!!!!!!!!!!!!!!!
## You CANNOT use distance metrics on non-numeric data
## Before we can proceed - we need to REMOVE
## all non-numeric columns

HeartDF2_num <- HeartDF2[,c(2,3,4,5,6)]
head(HeartDF2_num)


# Dissimilarity matrix with Euclidean
## dist in R
##  "euclidean", "maximum", "manhattan", 
## "canberra", "binary" or "minkowski" with p
(dE <- dist(HeartDF2_num, method = "euclidean"))
(dM <- dist(HeartDF2_num, method = "manhattan"))
(dMp2 <- dist(HeartDF2_num, method = "minkowski", p=2))

# Hierarchical clustering using Complete Linkage
(hc_C <- hclust(dM, method = "complete" ))
plot(hc_C)
## RE:
#https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/hclust
# Hierarchical clustering with Ward
# ward.D2" = Ward's minimum variance method -
# however dissimilarities are **squared before clustering. 
# "single" = Nearest neighbours method. 
# "complete" = distance between two clusters is defined 
# as the maximum distance between an observation in one.
hc_D <- hclust(dE, method = "ward.D" )
plot(hc_D)
hc_D2 <- hclust(dMp2, method = "ward.D2" )
plot(hc_D2)

######## Have a look ..........................
plot(hc_D2)
plot(hc_D)
plot(hc_C)

```

```{r}

##################################################
##
## Which methods to use??
##
## Method with stronger clustering structures??
######################################################
library(purrr)
#install.packages("cluster")

library(cluster)

methods <- c( "average", "single", "complete", "ward")
names(methods) <- c( "average", "single", "complete", "ward")
                     

# function to compute coefficient
MethodMeasures <- function(x) {
  cluster::agnes(HeartDF2_num, method = x)$ac
}

# The agnes() function will get the agglomerative coefficient (AC), 
# which measures the amount of clustering structure found.
# Get agglomerative coefficient for each linkage method
(purrr::map_dbl(methods, MethodMeasures))
#average    single  complete      ward 
#0.9629655 0.9642673 0.9623190 0.9645178 
# We can see that single is best in this case


############################################
## More on Determining optimal clusters
#######################################################
library("factoextra")
# Look at optimal cluster numbers using silh, elbow, gap
(WSS <- fviz_nbclust(HeartDF2_num, FUN = hcut, method = "wss", 
                   k.max = 5) +
  ggtitle("WSS:Elbow"))
SIL <- fviz_nbclust(HeartDF2_num, FUN = hcut, method = "silhouette", 
                   k.max = 5) +
  ggtitle("Silhouette")
GAP <- fviz_nbclust(HeartDF2_num, FUN = hcut, method = "gap_stat", 
                   k.max = 5) +
  ggtitle("Gap Stat")

# Display plots side by side
gridExtra::grid.arrange(WSS, SIL, GAP, nrow = 1)

############ and ...............
library(factoextra)
file2<-"C:/Users/profa/Documents/RStudioFolder_1/DrGExamples/ANLY503/HeartRisk.csv"
#data
# https://drive.google.com/file/d/1pt-ouIQXH-SQzUMSqbl6Z6UWZrY3i4qu/view?usp=sharing
HeartDF_no_outliers<-read.csv(filename)
head(HeartDF_no_outliers)
## Remove non-numbers
HeartDF_no_outliers_num<-HeartDF_no_outliers[,c(3,5,6)]
head(HeartDF_no_outliers_num)

## Use a distance metric
Dist_E<-dist(HeartDF_no_outliers_num, method = "euclidean" )
fviz_dist(Dist_E)

## If we change the row numbers to labels - we can SEE the clusters...
head(HeartDF_no_outliers)
## Save the first column of labels as names....
(names<-HeartDF_no_outliers[,c(1)])
str(names)
(names<-as.character(names))
## Here is an issue - row names need to be unique. 
## So - we need to append numbers to each to make them unique...
(names<-make.unique(names, sep = ""))
## set the row names of the HeartDF_no_outliers_num as these label names...
## What are they now?
row.names(HeartDF_no_outliers_num)
## Change them-->
(.rowNamesDF(HeartDF_no_outliers_num, make.names=FALSE) <- names)
##check
row.names(HeartDF_no_outliers_num)

## OK!! Fun tricks! Now - let's cluster again....
Dist_E<-dist(HeartDF_no_outliers_num, method = "euclidean" )
fviz_dist(Dist_E)



```

```{r}
#################
##
##  Density Based Clustering
##  -  BDSCAN - 
##
####################################################
library(fpc)
library(dbscan)

df <- Record_3D_DF
db <- fpc::dbscan(df, eps = 0.15, MinPts = 5)
# Plot DBSCAN results
plot(db, df, main = "DBSCAN")
plot(db, df$NA_Sales, main = "NA vs JP") 


```
```{r}
km_res <- kmeans(Record_3D_DF, 3, nstart = 25)
plot(Record_3D_DF, col=km_res$cluster+1, main="K-means")

```
```{r}
dbscan_res <- dbscan(Record_3D_DF, eps = 0.15)
plot(Record_3D_DF, col=dbscan_res$cluster+1, main="DBSCAN")
plot(Record_3D_DF$NA_Sales, col=dbscan_res$cluster, main="DBSCAN")

```
# -*- coding: utf-8 -*-
"""
Created on Wed Dec 11 03:14:52 2024

@author: joelh
"""
import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
import matplotlib
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn import preprocessing
from sklearn.model_selection import train_test_split

df = pd.read_csv("C:/Users/joelh/OneDrive/Desktop/CSCI5622/Proj/vgsales.csv", index_col=0)
df.head(20)

na = df.isna().sum().sort_values(ascending = False)
colna = na[na > 0]

perc = (round((colna/len(df)),4) * 100).map('{:.2f}%'.format)

naval = pd.concat([colna, perc], axis=1).rename(columns = {0: 'Sum of missing values', 1: 'Percentage of total'})
naval.head()

colna.head()

scaler = StandardScaler()

ClusterDF = df.dropna()
ClusterDF = ClusterDF.drop(['Name','Platform','Genre','Publisher'], axis=1)
ClusterDF.head(20)


My_KMean= KMeans(n_clusters=3)
My_KMean.fit(ClusterDF)
My_labels=My_KMean.predict(ClusterDF)
print(My_labels)

My_KMean2 = KMeans(n_clusters=3).fit(preprocessing.normalize(ClusterDF))
My_KMean2.fit(ClusterDF)
My_labels2=My_KMean2.predict(ClusterDF)
print(My_labels2)

My_KMean3= KMeans(n_clusters=3)
My_KMean3.fit(ClusterDF)
My_labels3=My_KMean3.predict(ClusterDF)
print("Silhouette Score for k = 3 \n",silhouette_score(ClusterDF, My_labels3))



TrainDF, TestDF = train_test_split(ClusterDF, test_size=0.3)
print(TrainDF)
print(TestDF)



from sklearn.cluster import KMeans
k_means=KMeans(n_clusters=2,random_state=42)
k_means.fit(TrainDF)
TrainDF['KMeans_labels']=k_means.labels_

# Plotting resulting clusters
colors=['purple','red','blue','green']
plt.figure(figsize=(10,10))
plt.scatter(TrainDF[2],TrainDF[3],c=df['KMeans_labels'],cmap=matplotlib.colors.ListedColormap(colors),s=15)
plt.title('K-Means Clustering',fontsize=20)
plt.xlabel('NA Sales',fontsize=14)
plt.ylabel('Jp Sales',fontsize=14)
plt.show()
